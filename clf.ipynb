{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d43b39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install all depedencies\n",
    "# a GPU is needed, so run this notebook in a cuda working environment\n",
    "\n",
    "!pip install -r ./src/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2e27b6",
   "metadata": {},
   "source": [
    "### DEEP Assisted Tagging Tool\n",
    "\n",
    "\n",
    "In this notebook we propose an example of the main model architecture for the assisted tagging tool that will soon be implemented in [**The DEEP**](https://thedeep.io/) platform.\n",
    "\n",
    "Let's recap for completeness what The DEEP is, and how Machine Learning models improve its use. \n",
    "\n",
    "The DEEP is a collaborative platform for qualitative data analysis supporting humanitarian analytical teams to produce actionable insights. Since its inception in the aftermath of the 2015 Nepal Earthquake, DEEP has significantly contributed to improving the humanitarian data ecosystem, and today, without a doubt, is the largest repository of annotated humanitarian response documents: 50k+ sources/leads and 400k+ entries, used for 300+ projects by 3.5k+ registered users in 60+ countries.\n",
    "\n",
    "During crises, rapidly identifying important information from available data (news, reports, research, etc.) is crucial to understanding the needs of affected populations and to improving evidence-based decision-making. To make the information classification process even faster, DEEP is largely benefitting from  Natural Language Processing (NLP) and Deep Learning (DL) to aid and support the manual tagging process and give the humanitarian community more time to produce analyses and take rapid action to save more lives.\n",
    "\n",
    "Up to now, all the information (of any kind: reports, news, articles, maps, infographics, etc.) uploaded to the platform has been annotated by hand by experts in the humanitarian sector. The tagging was done under several projects according to different predefined multi-label categories (analytical frameworks). Since the data is mostly textual, we internally developed NLP models that could improve and speed up the analysis of the texts. \n",
    "\n",
    "We must also consider that informations are often contained within document reports (PDF, docx etc.) of numerous pages, making the tagging effort very difficult and time-consuming, therefore we understand how important it can be to optimize the humanitarian response during, for example, an ongoing natural disaster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533325ec",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "Let's go into the details of the model now, starting from the data.\n",
    "\n",
    "In The DEEP platform each user or group has the possibility to create a project, which is usually link to a certain humanitarian crisis, such as a natural disaster, or to a certain geographic region or state where a rapid response is needed. Users can create custom labels and use them to annotate the information that will be uploaded within each project. Therefore each user will have the possibility to upload, for example, a document (of any format), select an exerpt of text (which perhaps contains important details for the purpose of the analysis) and annotate it using its own project labels. \n",
    "\n",
    "To combine entries from those projects and various analytical frameworks (set of labels), we defined a generic analytical framework and we transformed our labels accordingly. Our generic analytical framework has 10 different multi-label categories, totalling 86 different labels, covering all areas of a detailed humanitarian analysis.\n",
    "\n",
    "Our proposed dataset contains 8 categories overall:\n",
    "- 3 **primary tags**: sectors, pillars/subpillars_2d, pillars/subpillars_1d\n",
    "- 5 **secondary tags**: affected_groups, demographic_groups, specific_needs_groups, severity, geolocation\n",
    "\n",
    "Different tags are treated independently one from another. One model is trained alone for each different tag.\n",
    "\n",
    "In this notebook we focus only on a subset of above categories, **primary tags**.\n",
    "Primary tags contain 75 labels under different subcategories named as follows: \n",
    "- **Sectors** with 11 labels,\n",
    "- **2D Pillars** with  6 labels,\n",
    "- **2D Sub-pillars** with  18 labels,\n",
    "- **1D Pillars** with  7 labels,\n",
    "- **1D Sub-pillars** with  33 labels\n",
    "\n",
    "Let's see how they are divided:\n",
    "\n",
    "![image info](./img/plot1.png)\n",
    "\n",
    "We can see that, apart from Sectors, each subcategory has an annotation hierarchy, from Pillar to Sub-pillar (1D and 2D) . Furthermore, each text excerpt can be annotated with multiple labels, thus making the problem a multi-label text classification.\n",
    "\n",
    "The difference between 1D and 2D Pillars (and respective Sub-pillars), as we can see in the previous image, lies in the fact that the 2D subcategory presents an additional level of hierarchy, given by the Sectors. Example:\n",
    "\n",
    "![image info](./img/ex.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d712e645",
   "metadata": {},
   "source": [
    "#### Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b8e807",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dowload_dataset import download_file_from_google_drive\n",
    "\n",
    "# download public data from google drive\n",
    "\n",
    "TRAIN_VAL_ID = \"1W9-IN0DgYYsiwrWi9Aefyc1V87_q_LWd\"\n",
    "TEST_ID = \"11NQCwixCW2ZiFlSp1QNen2uiS2gThu2B\"\n",
    "\n",
    "# set files path for training and testing set\n",
    "TRAIN_VAL_NAME = \"train_val_dataset.csv\"\n",
    "TEST_NAME = \"test_dataset.csv\"\n",
    "\n",
    "# downloading\n",
    "download_file_from_google_drive(id=TRAIN_VAL_ID, destination=TRAIN_VAL_NAME)\n",
    "download_file_from_google_drive(id=TEST_ID, destination=TEST_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c056a24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv(TRAIN_VAL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b75f0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore the data\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783509b6",
   "metadata": {},
   "source": [
    "Textual entries are in 3 different languages: **English**, **Spanish** and **French**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bf8808",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "The model developed is based on pre-trained transformer architecture. The transformer had to fulfill some criteria:\n",
    "- **multilingual** : it needs to work for different languages\n",
    "- **good performance** : in order for it to be useful, the model needs to be performant\n",
    "- **fast predictions** : the main goal of the modelling is to give live predictions to taggers while they are working on tagging. Speed is critical in this case and the faster the model the better.\n",
    "- **one endpoint only for deployment**: in order to optimize costs, we want to have one endpoint only for all models and predictions. To do this, we create one custom class containing models and deploy it.\n",
    "\n",
    "We use the transformer [**microsoft/xtremedistil-l6-h256-uncased**](https://huggingface.co/microsoft/xtremedistil-l6-h256-uncased) as a backbone\n",
    "\n",
    "In this notebook we overall train three independent models: one for sectors, one for subpillars (1D and 2D) and one for secondary. \n",
    "Sectors is trained with a MLP-like standard architecture.\n",
    "\n",
    "For the subpillars tags, we use a tree-like multi-task learning model, fine-tuning the last hidden state of the transformer differently for each subtask. We have 13 different subtasks for the subpillars model (Humanitarian Conditions, At Risk, Displacement, Covid-19, Humanitarian Access, Impact, Information And Communication, Shock/Event, Capacities & Response, Context, Casualties, Priority Interventions, Priority Needs) each of which then has its own final labels, which we want to predict.\n",
    "This allows us to share the encoding information, obtained from our transformer backbone, and then train different heads separately depending on the hierarchy, further annealing the problems due to data imbalance and also some subtle differences between labels. \n",
    "\n",
    "The 5 first hidden layers from the backbone are common to all the tasks and the last hidden layer is specific to each task. We have, in total, 13 different sub-tasks for the subpillars model, for a total of 51 different tags.\n",
    "\n",
    "After training and to optimize our results, we hypertune the threshold for each label to optimize the f1 score on the validation set. Therefore, each different label has a different threshold. This keeps the models from overtagging and helps adapt to the data imbalanceness problem.\n",
    "\n",
    "In secondary tags we don't have a hierarchy, so the submodel is only a multi-task architecture\n",
    "\n",
    "![image info](./img/model.png)\n",
    "\n",
    "\n",
    "Now let's get started with the serious stuff ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d78166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use pytorch and pytorch-lightning as main frameworks\n",
    "\n",
    "import torch\n",
    "import copy, os\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "from typing import Optional\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn import metrics\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from transformers import AdamW\n",
    "\n",
    "# importing some utilities methods\n",
    "from src.utils import *\n",
    "# loss\n",
    "from src.loss import FocalLoss\n",
    "# encoder embeddings pooling module\n",
    "from src.pooling import Pooling\n",
    "# pytorch Dataset custom dataset builder\n",
    "from src.data import CustomDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5be607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the classifier architecture\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name_or_path: str,\n",
    "        ids_each_level,\n",
    "        dropout_rate=0.3,\n",
    "        output_length=384,\n",
    "        dim_hidden_layer: int = 256,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.ids_each_level = ids_each_level\n",
    "        self.l0 = AutoModel.from_pretrained(model_name_or_path)\n",
    "        self.pool = Pooling(word_embedding_dimension=output_length, pooling_mode=\"cls\")\n",
    "\n",
    "        self.LayerNorm_backbone = torch.nn.LayerNorm(output_length)\n",
    "        self.LayerNorm_specific_hidden = torch.nn.LayerNorm(dim_hidden_layer)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.specific_hidden_layer = [\n",
    "            torch.nn.Linear(output_length, dim_hidden_layer) for _ in ids_each_level\n",
    "        ]\n",
    "        self.specific_hidden_layer = torch.nn.ModuleList(self.specific_hidden_layer)\n",
    "\n",
    "        self.output_layer = [\n",
    "            torch.nn.Linear(dim_hidden_layer, len(id_one_level))\n",
    "            for id_one_level in ids_each_level\n",
    "        ]\n",
    "        self.output_layer = torch.nn.ModuleList(self.output_layer)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        \"\"\"\n",
    "        Multi-task forward.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        output = self.l0(\n",
    "            inputs[\"ids\"],\n",
    "            attention_mask=inputs[\"mask\"],\n",
    "        )\n",
    "        output = self.pool(\n",
    "            {\n",
    "                \"token_embeddings\": output.last_hidden_state,\n",
    "                \"attention_mask\": inputs[\"mask\"],\n",
    "            }\n",
    "        )[\"sentence_embedding\"]\n",
    "\n",
    "        last_hidden_states = [output.clone() for _ in self.ids_each_level]\n",
    "\n",
    "        heads = []\n",
    "        for i in range(len(self.ids_each_level)):\n",
    "            # specific hidden layer\n",
    "            output_tmp = F.selu(last_hidden_states[i])\n",
    "            output_tmp = self.dropout(output_tmp)\n",
    "            output_tmp = self.LayerNorm_specific_hidden(output_tmp)\n",
    "\n",
    "            # output layer\n",
    "            output_tmp = self.output_layer[i](output_tmp)\n",
    "            heads.append(output_tmp)\n",
    "\n",
    "        return heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f482890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch-lightining model class\n",
    "# as loss we use a BCE focal loss (details in ./src/loss.py)\n",
    "\n",
    "class Transformer(pl.LightningModule):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name_or_path: str,\n",
    "        train_dataset,\n",
    "        val_dataset,\n",
    "        train_params,\n",
    "        val_params,\n",
    "        tokenizer,\n",
    "        column_name,\n",
    "        multiclass,\n",
    "        learning_rate: float = 1e-5,\n",
    "        adam_epsilon: float = 1e-7,\n",
    "        warmup_steps: int = 500,\n",
    "        weight_decay: float = 0.1,\n",
    "        train_batch_size: int = 32,\n",
    "        eval_batch_size: int = 32,\n",
    "        dropout_rate: float = 0.3,\n",
    "        max_len: int = 512,\n",
    "        output_length: int = 384,\n",
    "        training_device: str = \"cuda\",\n",
    "        keep_neg_examples: bool = False,\n",
    "        dim_hidden_layer: int = 256,\n",
    "        **kwargs,\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "        self.output_length = output_length\n",
    "        self.column_name = column_name\n",
    "        self.save_hyperparameters()\n",
    "        self.targets = train_dataset[\"target\"]\n",
    "        self.tagname_to_tagid = tagname_to_id(train_dataset[\"target\"])\n",
    "        self.num_labels = len(self.tagname_to_tagid)\n",
    "        self.get_first_level_ids()\n",
    "\n",
    "        self.max_len = max_len\n",
    "        self.model = Model(\n",
    "            model_name_or_path,\n",
    "            self.ids_each_level,\n",
    "            dropout_rate,\n",
    "            self.output_length,\n",
    "            dim_hidden_layer,\n",
    "        )\n",
    "        self.tokenizer = tokenizer\n",
    "        self.val_params = val_params\n",
    "\n",
    "        self.training_device = training_device\n",
    "\n",
    "        self.multiclass = multiclass\n",
    "        self.keep_neg_examples = keep_neg_examples\n",
    "\n",
    "        self.training_loader = self.get_loaders(\n",
    "            train_dataset, train_params, self.tagname_to_tagid, self.tokenizer, max_len\n",
    "        )\n",
    "        self.val_loader = self.get_loaders(\n",
    "            val_dataset, val_params, self.tagname_to_tagid, self.tokenizer, max_len\n",
    "        )\n",
    "        self.Focal_loss = FocalLoss()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        output = self.model(inputs)\n",
    "        return output\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outputs = self(batch)\n",
    "        train_loss = self.get_loss(outputs, batch[\"targets\"])\n",
    "\n",
    "        self.log(\n",
    "            \"train_loss\", train_loss.item(), prog_bar=True, on_step=False, on_epoch=True\n",
    "        )\n",
    "        return train_loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        outputs = self(batch)\n",
    "        val_loss = self.get_loss(outputs, batch[\"targets\"])\n",
    "        self.log(\n",
    "            \"val_loss\",\n",
    "            val_loss,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=False,\n",
    "        )\n",
    "\n",
    "        return {\"val_loss\": val_loss}\n",
    "\n",
    "    def total_steps(self) -> int:\n",
    "        \"\"\"The number of total training steps that will be run. Used for lr scheduler purposes.\"\"\"\n",
    "        self.dataset_size = len(self.train_dataloader().dataset)\n",
    "        num_devices = max(1, self.hparams.gpus)\n",
    "        effective_batch_size = (\n",
    "            self.hparams.train_batch_size\n",
    "            * self.hparams.accumulate_grad_batches\n",
    "            * num_devices\n",
    "        )\n",
    "        return (self.dataset_size / effective_batch_size) * self.hparams.max_epochs\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"Prepare optimizer and schedule (linear warmup and decay)\"\n",
    "\n",
    "        optimizer = AdamW(\n",
    "            self.parameters(),\n",
    "            lr=self.hparams.learning_rate,\n",
    "            weight_decay=self.hparams.weight_decay,\n",
    "            eps=self.hparams.adam_epsilon,\n",
    "        )\n",
    "\n",
    "        scheduler = ReduceLROnPlateau(\n",
    "            optimizer, \"min\", 0.5, patience=self.hparams.max_epochs // 6\n",
    "        )\n",
    "        scheduler = {\n",
    "            \"scheduler\": scheduler,\n",
    "            \"interval\": \"epoch\",\n",
    "            \"frequency\": 1,\n",
    "            \"monitor\": \"val_loss\",\n",
    "        }\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.training_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.val_loader\n",
    "\n",
    "    def get_loaders(\n",
    "        self, dataset, params, tagname_to_tagid, tokenizer, max_len: int = 128\n",
    "    ):\n",
    "\n",
    "        set = CustomDataset(dataset, tagname_to_tagid, tokenizer, max_len)\n",
    "        loader = DataLoader(set, **params, pin_memory=True)\n",
    "        return loader\n",
    "\n",
    "    def get_loss(self, outputs, targets, only_pos: bool = False):\n",
    "\n",
    "        # keep the if because we want to take negative examples into account for the models that contain\n",
    "        # no hierarchy (upper level models)\n",
    "        \n",
    "        if len(self.ids_each_level) == 1:\n",
    "            return self.Focal_loss(outputs[0], targets)\n",
    "        else:\n",
    "            tot_loss = 0\n",
    "            for i_th_level in range(len(self.ids_each_level)):\n",
    "                ids_one_level = self.ids_each_level[i_th_level]\n",
    "                outputs_i_th_level = outputs[i_th_level]\n",
    "                targets_one_level = targets[:, ids_one_level]\n",
    "                \n",
    "                # main objective: for each level, if row contains only zeros, not to do backpropagation\n",
    "\n",
    "                if only_pos:\n",
    "                    mask_ids_neg_example = [\n",
    "                        not bool(int(torch.sum(one_row)))\n",
    "                        for one_row in targets_one_level\n",
    "                    ]\n",
    "                    outputs_i_th_level[mask_ids_neg_example, :] = 1e-8\n",
    "\n",
    "                tot_loss += self.Focal_loss(outputs_i_th_level, targets_one_level)\n",
    "\n",
    "            return tot_loss\n",
    "\n",
    "    def get_first_level_ids(self):\n",
    "        \n",
    "        all_names = list(self.tagname_to_tagid.keys())\n",
    "        if np.all([\"->\" in name for name in all_names]):\n",
    "            first_level_names = list(\n",
    "                np.unique([name.split(\"->\")[0] for name in all_names])\n",
    "            )\n",
    "            self.ids_each_level = [\n",
    "                [i for i in range(len(all_names)) if name in all_names[i]]\n",
    "                for name in first_level_names\n",
    "            ]\n",
    "\n",
    "        else:\n",
    "            self.ids_each_level = [[i for i in range(len(all_names))]]\n",
    "\n",
    "    def custom_predict(\n",
    "        self, validation_dataset, testing=False, hypertuning_threshold: bool = False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        1) get raw predictions\n",
    "        2) postprocess them to output an output compatible with what we want in the inference\n",
    "        \"\"\"\n",
    "\n",
    "        def sigmoid(x):\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "\n",
    "        if testing:\n",
    "            self.val_params[\"num_workers\"] = 0\n",
    "\n",
    "        validation_loader = self.get_loaders(\n",
    "            validation_dataset,\n",
    "            self.val_params,\n",
    "            self.tagname_to_tagid,\n",
    "            self.tokenizer,\n",
    "            self.max_len,\n",
    "        )\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            testing_device = \"cuda\"\n",
    "        else:\n",
    "            testing_device = \"cpu\"\n",
    "\n",
    "        self.to(testing_device)\n",
    "        self.eval()\n",
    "        self.freeze()\n",
    "        y_true = []\n",
    "        logit_predictions = []\n",
    "        indexes = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(\n",
    "                validation_loader,\n",
    "                total=len(validation_loader.dataset) // validation_loader.batch_size,\n",
    "            ):\n",
    "\n",
    "                if not testing:\n",
    "                    y_true.append(batch[\"targets\"].detach())\n",
    "                    indexes.append(batch[\"entry_id\"].detach())\n",
    "\n",
    "                logits = self(\n",
    "                    {\n",
    "                        \"ids\": batch[\"ids\"].to(testing_device),\n",
    "                        \"mask\": batch[\"mask\"].to(testing_device),\n",
    "                        \"token_type_ids\": batch[\"token_type_ids\"].to(testing_device),\n",
    "                    }\n",
    "                )\n",
    "                logits = torch.cat(logits, dim=1)  # have a matrix like in the beginning\n",
    "                logits_to_array = np.array([np.array(t) for t in logits.cpu()])\n",
    "                logit_predictions.append(logits_to_array)\n",
    "\n",
    "        logit_predictions = np.concatenate(logit_predictions)\n",
    "        logit_predictions = sigmoid(logit_predictions)\n",
    "\n",
    "        target_list = list(self.tagname_to_tagid.keys())\n",
    "        probabilities_dict = []\n",
    "        # postprocess predictions\n",
    "        for i in range(logit_predictions.shape[0]):\n",
    "\n",
    "            # Return predictions\n",
    "            # row_pred = np.array([0] * self.num_labels)\n",
    "            row_logits = logit_predictions[i, :]\n",
    "\n",
    "            # Return probabilities\n",
    "            probabilities_item_dict = {}\n",
    "            for j in range(logit_predictions.shape[1]):\n",
    "                if hypertuning_threshold:\n",
    "                    probabilities_item_dict[target_list[j]] = row_logits[j]\n",
    "                else:\n",
    "                    probabilities_item_dict[target_list[j]] = (\n",
    "                        row_logits[j] / self.optimal_thresholds[target_list[j]]\n",
    "                    )\n",
    "\n",
    "            probabilities_dict.append(probabilities_item_dict)\n",
    "\n",
    "        if not testing:\n",
    "            y_true = np.concatenate(y_true)\n",
    "            indexes = np.concatenate(indexes)\n",
    "            return indexes, logit_predictions, y_true, probabilities_dict\n",
    "\n",
    "        else:\n",
    "            return probabilities_dict\n",
    "\n",
    "    def hypertune_threshold(self, beta_f1: float = 0.8):\n",
    "        \"\"\"\n",
    "        having the probabilities, loop over a list of thresholds to see which one:\n",
    "        1) yields the best results\n",
    "        2) without being an aberrant value\n",
    "        \"\"\"\n",
    "\n",
    "        data_for_threshold_tuning = self.val_loader.dataset.data\n",
    "        indexes, logit_predictions, y_true, _ = self.custom_predict(\n",
    "            data_for_threshold_tuning, hypertuning_threshold=True\n",
    "        )\n",
    "\n",
    "        thresholds_list = np.linspace(0.0, 1.0, 101)[::-1]\n",
    "        optimal_thresholds_dict = {}\n",
    "        optimal_scores = []\n",
    "        for ids_one_level in self.ids_each_level:\n",
    "            y_true_one_level = y_true[:, ids_one_level]\n",
    "            logit_preds_one_level = logit_predictions[:, ids_one_level]\n",
    "\n",
    "            \"\"\"if len(self.ids_each_level) > 1: #multitask\n",
    "\n",
    "                mask_at_least_one_pos = [bool(sum(row)) for row in y_true_one_level]\n",
    "                threshold_tuning_gt = y_true_one_level[mask_at_least_one_pos]\n",
    "                threshold_tuning_logit_preds = logit_preds_one_level[mask_at_least_one_pos]\n",
    "            else: #no multitask\n",
    "                threshold_tuning_gt = y_true_one_level\n",
    "                threshold_tuning_logit_preds = logit_predictions\n",
    "\n",
    "            assert(threshold_tuning_logit_preds.shape == threshold_tuning_gt.shape)\"\"\"\n",
    "\n",
    "            for j in range(len(ids_one_level)):\n",
    "                scores = []\n",
    "                for thresh_tmp in thresholds_list:\n",
    "                    metric = self.get_metric(\n",
    "                        logit_preds_one_level,\n",
    "                        y_true_one_level,\n",
    "                        beta_f1,\n",
    "                        j,\n",
    "                        thresh_tmp,\n",
    "                    )\n",
    "                    scores.append(metric)\n",
    "\n",
    "                max_threshold = 0\n",
    "                max_score = 0\n",
    "                for i in range(1, len(scores) - 1):\n",
    "                    score = np.mean(scores[i - 1 : i + 2])\n",
    "                    if score >= max_score:\n",
    "                        max_score = score\n",
    "                        max_threshold = thresholds_list[i]\n",
    "\n",
    "                optimal_scores.append(max_score)\n",
    "\n",
    "                optimal_thresholds_dict[\n",
    "                    list(self.tagname_to_tagid.keys())[ids_one_level[j]]\n",
    "                ] = max_threshold\n",
    "\n",
    "        self.optimal_thresholds = optimal_thresholds_dict\n",
    "\n",
    "        return np.mean(optimal_scores)\n",
    "\n",
    "    def get_metric(self, preds, groundtruth, beta_f1, column_idx, threshold_tmp):\n",
    "        columns_logits = np.array(preds[:, column_idx])\n",
    "\n",
    "        column_pred = [\n",
    "            1 if one_logit > threshold_tmp else 0 for one_logit in columns_logits\n",
    "        ]\n",
    "\n",
    "        if self.multiclass:\n",
    "            metric = metrics.fbeta_score(\n",
    "                groundtruth[:, column_idx],\n",
    "                column_pred,\n",
    "                beta_f1,\n",
    "                average=\"macro\",\n",
    "            )\n",
    "        else:\n",
    "            metric = metrics.f1_score(\n",
    "                groundtruth[:, column_idx],\n",
    "                column_pred,\n",
    "                average=\"macro\",\n",
    "            )\n",
    "        return metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149c2bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main class used to train model\n",
    "    \n",
    "class CustomTrainer:\n",
    "\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_dataset,\n",
    "        val_dataset,\n",
    "        training_column,\n",
    "        MODEL_DIR: str,\n",
    "        MODEL_NAME: str,\n",
    "        TOKENIZER_NAME: str,\n",
    "        dropout_rate: float,\n",
    "        train_params,\n",
    "        val_params,\n",
    "        gpu_nb: int,\n",
    "        MAX_EPOCHS: int,\n",
    "        weight_decay=0.02,\n",
    "        warmup_steps=500,\n",
    "        output_length=384,\n",
    "        max_len=150,\n",
    "        multiclass_bool=True,\n",
    "        keep_neg_examples_bool=False,\n",
    "        learning_rate=3e-5,\n",
    "        weighted_loss: str = \"sqrt\",\n",
    "        training_device: str = \"cuda\",\n",
    "        beta_f1: float = 0.8,\n",
    "        dim_hidden_layer: int = 256\n",
    "    ) -> None:\n",
    "        \n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.training_column = training_column\n",
    "        self.MODEL_DIR = MODEL_DIR\n",
    "        self.MODEL_NAME = MODEL_NAME\n",
    "        self.TOKENIZER_NAME = TOKENIZER_NAME\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.train_params = train_params\n",
    "        self.val_params = val_params\n",
    "        self.gpu_nb = gpu_nb\n",
    "        self.MAX_EPOCHS = MAX_EPOCHS\n",
    "        self.weight_decay = weight_decay\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.output_length = output_length\n",
    "        self.max_len = max_len\n",
    "        self.multiclass_bool = multiclass_bool\n",
    "        self.keep_neg_examples_bool = keep_neg_examples_bool\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weighted_loss = weighted_loss\n",
    "        self.training_device = training_device\n",
    "        self.beta_f1 = beta_f1\n",
    "        self.dim_hidden_layer = dim_hidden_layer\n",
    "\n",
    "    def train_model(self):\n",
    "        PATH_NAME = self.MODEL_DIR\n",
    "        if not os.path.exists(PATH_NAME):\n",
    "            os.makedirs(PATH_NAME)\n",
    "\n",
    "        early_stopping_callback = EarlyStopping(\n",
    "            monitor=\"val_loss\", patience=2, mode=\"min\"\n",
    "        )\n",
    "\n",
    "        checkpoint_callback_params = {\n",
    "            \"save_top_k\": 1,\n",
    "            \"verbose\": True,\n",
    "            \"monitor\": \"val_loss\",\n",
    "            \"mode\": \"min\",\n",
    "        }\n",
    "\n",
    "        FILENAME = \"model_\" + self.training_column\n",
    "        dirpath_pillars = str(PATH_NAME)\n",
    "        checkpoint_callback = ModelCheckpoint(\n",
    "            dirpath=dirpath_pillars, filename=FILENAME, **checkpoint_callback_params\n",
    "        )\n",
    "\n",
    "        logger = TensorBoardLogger(\"lightning_logs\", name=FILENAME)\n",
    "\n",
    "        trainer = pl.Trainer(\n",
    "            logger=logger,\n",
    "            callbacks=[early_stopping_callback, checkpoint_callback],\n",
    "            progress_bar_refresh_rate=40,\n",
    "            profiler=\"simple\",\n",
    "            log_gpu_memory=True,\n",
    "            weights_summary=None,\n",
    "            gpus=self.gpu_nb,\n",
    "            precision=16,\n",
    "            accumulate_grad_batches=1,\n",
    "            max_epochs=self.MAX_EPOCHS,\n",
    "            gradient_clip_val=1,\n",
    "            gradient_clip_algorithm=\"norm\"\n",
    "        )\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.TOKENIZER_NAME)\n",
    "        \n",
    "        model = Transformer(\n",
    "            model_name_or_path=self.MODEL_NAME,\n",
    "            train_dataset=self.train_dataset,\n",
    "            val_dataset=self.val_dataset,\n",
    "            train_params=self.train_params,\n",
    "            val_params=self.val_params,\n",
    "            tokenizer=tokenizer,\n",
    "            column_name=self.training_column,\n",
    "            gpus=self.gpu_nb,\n",
    "            plugin=\"deepspeed_stage_3_offload\",\n",
    "            accumulate_grad_batches=1,\n",
    "            max_epochs=self.MAX_EPOCHS,\n",
    "            dropout_rate=self.dropout_rate,\n",
    "            weight_decay=self.weight_decay,\n",
    "            warmup_steps=self.warmup_steps,\n",
    "            output_length=self.output_length,\n",
    "            learning_rate=self.learning_rate,\n",
    "            multiclass=self.multiclass_bool,\n",
    "            weighted_loss=self.weighted_loss,\n",
    "            training_device=self.training_device,\n",
    "            keep_neg_examples=self.keep_neg_examples_bool,\n",
    "            dim_hidden_layer=self.dim_hidden_layer\n",
    "        )\n",
    "\n",
    "        \"\"\"lr_finder = trainer.tuner.lr_find(model)\n",
    "        new_lr = lr_finder.suggestion()\n",
    "        model.hparams.learning_rate = new_lr\"\"\"\n",
    "        trainer.fit(model)\n",
    "        model.train_f1_score = model.hypertune_threshold(self.beta_f1)\n",
    "\n",
    "        del model.training_loader\n",
    "        del model.val_loader\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1823358d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns selection to train\n",
    "# here we train only primary tags: sectors and pillars/sub-pillars (1D & 2D)\n",
    "\n",
    "columns = [\n",
    "    'excerpt', \n",
    "    'entry_id', \n",
    "    'subpillars', \n",
    "    'sectors' \n",
    "    #'secondary_tags' we don't train secondary tags here \n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf697fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp = {  \n",
    "    \"epochs\": 2,\n",
    "    \"train_batch_size\": 32,\n",
    "    \"val_batch_size\": 32,\n",
    "    \"max_len\": 512,\n",
    "    \"warmup_steps\": 10,\n",
    "    \"learning_rate\": 3e-5,\n",
    "    \"output_length\": 256,\n",
    "    \"nb_repetitions\": 1,\n",
    "    \"model_name\":  \"microsoft/xtremedistil-l6-h256-uncased\",\n",
    "    \"tokenizer_name\": \"microsoft/xtremedistil-l6-h256-uncased\",\n",
    "    \"beta_f1\": 0.7,   \n",
    "    \"n_gpu\": 1,\n",
    "    # set a directory name for trianed-models checkpoints\n",
    "    \"model_dir\": \"./models\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120cc49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(dataset, training_columns, hyperparaments):\n",
    "        \n",
    "    models = {}\n",
    "    \n",
    "    train_params = {\n",
    "        \"batch_size\": hyperparaments[\"train_batch_size\"],\n",
    "        \"shuffle\": True,\n",
    "        \"num_workers\": 4\n",
    "    }\n",
    "    \n",
    "    val_params = {\n",
    "        \"batch_size\": hyperparaments[\"val_batch_size\"],\n",
    "        \"shuffle\": False,\n",
    "        \"num_workers\": 4\n",
    "    }\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        gpu_nb = 1\n",
    "        training_device = \"cuda\"\n",
    "    else:\n",
    "        gpu_nb = 0\n",
    "        training_device = \"cpu\"\n",
    "        \n",
    "    print(f\"Training device: {training_device}\")\n",
    "        \n",
    "    for column in training_columns[2:]:\n",
    "        \n",
    "        multiclass_bool = column != \"severity\"\n",
    "        keep_neg_examples = True\n",
    "        \n",
    "        best_score = 0\n",
    "        iter_nb = 0\n",
    "\n",
    "        while best_score < 0.7 and iter_nb < hyperparaments[\"nb_repetitions\"]:\n",
    "\n",
    "            train_df, val_df = preprocess_df(\n",
    "                dataset, column, multiclass_bool, keep_neg_examples\n",
    "            )\n",
    "            print(f\"Data pre-processing. Train: {len(train_df)}, Validation: {len(val_df)}\")\n",
    "            \n",
    "            # for the purpose of our sample notebook we select a minimal number of epochs\n",
    "            max_epochs = 1\n",
    "\n",
    "            if len(train_df) > 120_000:\n",
    "                dropout_column = 0.2\n",
    "                weight_decay_col = 1e-3\n",
    "                dim_hidden_layer = 256\n",
    "                # max_epochs = 5\n",
    "                learning_rate = 8e-5\n",
    "\n",
    "            elif len(train_df) > 50_000:\n",
    "                dropout_column = 0.3\n",
    "                weight_decay_col = 3e-3\n",
    "                dim_hidden_layer = 256\n",
    "                # max_epochs = 8\n",
    "                learning_rate = 5e-5\n",
    "            else:\n",
    "                dropout_column = 0.3\n",
    "                weight_decay_col = 0.01\n",
    "                dim_hidden_layer = 256\n",
    "                # max_epochs = 12\n",
    "                learning_rate = 3e-5\n",
    "\n",
    "            if iter_nb == 1:\n",
    "                learning_rate = learning_rate * 0.7\n",
    "                weight_decay_col = weight_decay_col * 2\n",
    "                \n",
    "            \n",
    "            model_trainer = CustomTrainer(\n",
    "                    train_dataset=train_df,\n",
    "                    val_dataset=val_df,\n",
    "                    MODEL_DIR=hyperparaments[\"model_dir\"],\n",
    "                    MODEL_NAME=hyperparaments[\"model_name\"],\n",
    "                    TOKENIZER_NAME=hyperparaments[\"tokenizer_name\"],\n",
    "                    training_column=column,\n",
    "                    gpu_nb=gpu_nb,\n",
    "                    train_params=train_params,\n",
    "                    val_params=val_params,\n",
    "                    MAX_EPOCHS=max_epochs,\n",
    "                    dropout_rate=dropout_column,\n",
    "                    weight_decay=weight_decay_col,\n",
    "                    learning_rate=learning_rate,\n",
    "                    max_len=hyperparaments[\"max_len\"],\n",
    "                    warmup_steps=hyperparaments[\"warmup_steps\"],\n",
    "                    output_length=hyperparaments[\"output_length\"],\n",
    "                    multiclass_bool=multiclass_bool,\n",
    "                    training_device=training_device,\n",
    "                    beta_f1=hyperparaments[\"beta_f1\"],\n",
    "                    dim_hidden_layer=dim_hidden_layer\n",
    "                )\n",
    "        \n",
    "            model = model_trainer.train_model()\n",
    "            model_score = model.train_f1_score\n",
    "            \n",
    "            if model_score > best_score:\n",
    "                best_score = model_score\n",
    "                models.update({column: model})\n",
    "\n",
    "            iter_nb += 1\n",
    "    \n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74121925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run training\n",
    "\n",
    "models = run_training(dataset, training_columns=columns, hyperparaments=hyp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3aa228",
   "metadata": {},
   "source": [
    "### Get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0094ca71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also use resulting models in a second time loading from models checkpoints. Example:\n",
    "\n",
    "# cm_subpillars = Transformer.load_from_checkpoint(\"./models/model_subpillars.ckpt\")\n",
    "# cm_sectors = Transformer.load_from_checkpoint(\"./models/model_sectors.ckpt\")\n",
    "# cm_subpilllars.hypertune_threshold(beta_f1=hyp[\"beta_f1\"])\n",
    "# cm_sectors.hyptertune_threshold(beta_f1=hyp[\"beta_f1\"])\n",
    "\n",
    "# models = {\"subpillars\": cm_subpillars, \"sectors\": cm_sectors}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251a0f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = pd.read_csv(TEST_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54beedfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_predictions = {}\n",
    "\n",
    "for tag_name, trained_model in models.items():\n",
    "\n",
    "    predictions_one_model = trained_model.custom_predict(\n",
    "        test_dataset[\"excerpt\"], testing=True\n",
    "    )\n",
    "    raw_predictions[tag_name] = predictions_one_model\n",
    "\n",
    "outputs = {\n",
    "    \"preds\": raw_predictions,\n",
    "    \"thresholds\": trained_model.optimal_thresholds\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd48bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pillars_1d_tags = ['Covid-19',\n",
    " 'Casualties',\n",
    " 'Context',\n",
    " 'Displacement',\n",
    " 'Humanitarian Access',\n",
    " 'Shock/Event',\n",
    " 'Information And Communication']\n",
    "\n",
    "pillars_2d_tags = ['At Risk',\n",
    " 'Priority Interventions',\n",
    " 'Capacities & Response',\n",
    " 'Humanitarian Conditions',\n",
    " 'Impact',\n",
    " 'Priority Needs']\n",
    " \n",
    "def get_predictions_all(ratio_proba_threshold, \n",
    "    output_columns,\n",
    "    pillars_2d,\n",
    "    pillars_1d,\n",
    "    nb_entries: int, \n",
    "    ratio_nb: int):\n",
    "    \n",
    "    predictions = {column:[] for column in output_columns }\n",
    "    for entry_nb in range (nb_entries):\n",
    "        returns_sectors = ratio_proba_threshold['sectors'][entry_nb] \n",
    "        preds_sectors = get_preds_entry (returns_sectors, False, ratio_nb)  \n",
    "        predictions['sectors'].append(preds_sectors)\n",
    "        \n",
    "        returns_subpillars = ratio_proba_threshold['subpillars'][entry_nb] \n",
    "        \n",
    "        subpillars_2d_tags = {\n",
    "           key: value for key, value in returns_subpillars.items() if\\\n",
    "                key.split('->')[0] in pillars_2d\n",
    "        }\n",
    "        subpillars_1d_tags = {\n",
    "           key: value for key, value in returns_subpillars.items() if\\\n",
    "                key.split('->')[0] in pillars_1d\n",
    "        }\n",
    "        if len(preds_sectors)==0:\n",
    "            preds_2d = []\n",
    "        else:\n",
    "            preds_2d = get_preds_entry (subpillars_2d_tags, True, ratio_nb)\n",
    "        \n",
    "        predictions['subpillars_2d'].append(preds_2d)\n",
    "        \n",
    "        preds_1d = get_preds_entry (subpillars_1d_tags, False, ratio_nb)\n",
    "        predictions['subpillars_1d'].append(preds_1d)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def get_preds_entry (preds_column, return_at_least_one=True, ratio_nb=1, return_only_one=False):\n",
    "    preds_entry = [\n",
    "        sub_tag for sub_tag in list(preds_column.keys()) if preds_column[sub_tag]>ratio_nb\n",
    "    ]\n",
    "    if return_only_one:\n",
    "        preds_entry = [\n",
    "            sub_tag for sub_tag in list(preds_column.keys())\\\n",
    "                if preds_column[sub_tag]==max(list(preds_column.values()))\n",
    "        ]\n",
    "    if return_at_least_one:\n",
    "        if len(preds_entry)==0:\n",
    "            preds_entry = [\n",
    "                sub_tag for sub_tag in list(preds_column.keys())\\\n",
    "                    if preds_column[sub_tag]==max(list(preds_column.values()))\n",
    "            ]\n",
    "    return preds_entry\n",
    "\n",
    "final_preds = get_predictions_all(\n",
    "    outputs['preds'], #raw predictions returned\n",
    "    [ 'sectors', 'subpillars_2d', 'subpillars_1d'],\n",
    "    pillars_2d=pillars_2d_tags,\n",
    "    pillars_1d=pillars_1d_tags,\n",
    "    nb_entries=len(test_set), #total number of predictions to be postprocessed\n",
    "    ratio_nb=1)\n",
    "\n",
    "\n",
    "predictions_df = test_dataset[[\n",
    "    'excerpt', 'entry_id'\n",
    "]]\n",
    "\n",
    "predictions_df['sectors'] = final_preds['sectors']\n",
    "predictions_df['subpillars_2d'] = final_preds['subpillars_2d']\n",
    "predictions_df['subpillars_1d'] = final_preds['subpillars_1d']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c2f1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d793194f",
   "metadata": {},
   "source": [
    "Once you have the predictions it is easy to get the metrics you want against the targets in the test dataset.\n",
    "\n",
    "For completeness we report here our results, obtained with a larger number of epochs and data.\n",
    "\n",
    "![image info](./img/image.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepl",
   "language": "python",
   "name": "deepl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
